{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83329a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:20:59.212477Z",
     "start_time": "2023-06-19T17:20:54.051467Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from utils import match_loss, regularization, row_normalize_tensor\n",
    "import deeprobust.graph.utils as utils\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from models.gcn import GCN\n",
    "from models.sgc import SGC\n",
    "from models.sgc_multi import SGC as SGC1\n",
    "from models.parametrized_adj import PGE\n",
    "import scipy.sparse as sp\n",
    "from torch_sparse import SparseTensor\n",
    "import  torch, os\n",
    "import  numpy as np\n",
    "from    subgraph_data_processing import Subgraphs,Subgraphs_syn\n",
    "import  scipy.stats\n",
    "from    torch.utils.data import DataLoader\n",
    "from    torch.optim import lr_scheduler\n",
    "import  random, sys, pickle\n",
    "import  argparse\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "from itertools import combinations \n",
    "import networkx.algorithms.isomorphism as iso\n",
    "from tqdm import tqdm\n",
    "import dgl\n",
    "\n",
    "from meta import Meta\n",
    "import time\n",
    "import copy\n",
    "import psutil\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec8f797",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:20:59.235229Z",
     "start_time": "2023-06-19T17:20:59.212991Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', type=int, default=0, help='gpu id')\n",
    "parser.add_argument('--dataset', type=str, default='cora')\n",
    "parser.add_argument('--dis_metric', type=str, default='ours')\n",
    "parser.add_argument('--epochs', type=int, default=2000)\n",
    "parser.add_argument('--nlayers', type=int, default=3)\n",
    "parser.add_argument('--hidden', type=int, default=256)\n",
    "parser.add_argument('--lr_adj', type=float, default=0.01)\n",
    "parser.add_argument('--lr_feat', type=float, default=0.01)\n",
    "parser.add_argument('--lr_model', type=float, default=0.01)\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0)\n",
    "parser.add_argument('--dropout', type=float, default=0.0)\n",
    "parser.add_argument('--normalize_features', type=bool, default=True)\n",
    "parser.add_argument('--keep_ratio', type=float, default=1.0)\n",
    "parser.add_argument('--reduction_rate', type=float, default=1)\n",
    "parser.add_argument('--seed', type=int, default=15, help='Random seed.')\n",
    "parser.add_argument('--alpha', type=float, default=0, help='regularization term.')\n",
    "parser.add_argument('--debug', type=int, default=0)\n",
    "parser.add_argument('--sgc', type=int, default=1)\n",
    "parser.add_argument('--inner', type=int, default=0)\n",
    "parser.add_argument('--outer', type=int, default=20)\n",
    "parser.add_argument('--save', type=int, default=0)\n",
    "parser.add_argument('--one_step', type=int, default=0)\n",
    "\n",
    "parser.add_argument('--epoch', type=int, help='epoch number', default=10)\n",
    "parser.add_argument('--n_way', type=int, help='n way', default=3)\n",
    "parser.add_argument('--k_spt', type=int, help='k shot for support set', default=3)\n",
    "parser.add_argument('--k_qry', type=int, help='k shot for query set', default=24)\n",
    "parser.add_argument('--task_num', type=int, help='meta batch size, namely task num', default=32)\n",
    "parser.add_argument('--meta_lr', type=float, help='meta-level outer learning rate', default=1e-3)\n",
    "parser.add_argument('--update_lr', type=float, help='task-level inner update learning rate', default=1e-2)\n",
    "parser.add_argument('--update_step', type=int, help='task-level inner update steps', default=10)\n",
    "parser.add_argument('--update_step_test', type=int, help='update steps for finetunning', default=20)\n",
    "parser.add_argument('--input_dim', type=int, help='input feature dim', default=1)\n",
    "parser.add_argument('--hidden_dim', type=int, help='hidden dim', default=256)\n",
    "parser.add_argument('--attention_size', type=int, help='dim of attention_size', default=32)\n",
    "parser.add_argument(\"--data_dir\", default=\"D:\\pythonProject\\python_file\\Graph_DD\\G-Meta-master\\arxiv\", type=str, required=False, help=\"The input data dir.\")\n",
    "parser.add_argument(\"--no_finetune\", default=True, type=str, required=False, help=\"no finetune mode.\")\n",
    "parser.add_argument(\"--task_setup\", default='Disjoint', type=str, required=False, help=\"Select from Disjoint or Shared Setup. For Disjoint-Label, single/multiple graphs are both considered.\")\n",
    "parser.add_argument(\"--method\", default='G-Meta', type=str, required=False, help=\"Use G-Meta\")\n",
    "parser.add_argument('--task_n', type=int, help='task number', default=1)\n",
    "parser.add_argument(\"--task_mode\", default='False', type=str, required=False, help=\"For Evaluating on Tasks\")\n",
    "parser.add_argument(\"--val_result_report_steps\", default=100, type=int, required=False, help=\"validation report\")\n",
    "parser.add_argument(\"--train_result_report_steps\", default=200, type=int, required=False, help=\"training report\")\n",
    "parser.add_argument(\"--num_workers\", default=0, type=int, required=False, help=\"num of workers\")\n",
    "parser.add_argument(\"--batchsz\", default=10000, type=int, required=False, help=\"batch size\")\n",
    "parser.add_argument(\"--link_pred_mode\", default='False', type=str, required=False, help=\"For Link Prediction\")\n",
    "parser.add_argument(\"--h\", default=2, type=int, required=False, help=\"neighborhood size\")\n",
    "parser.add_argument('--sample_nodes', type=int, help='sample nodes if above this number of nodes', default=1000)\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "693c5de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:20:59.258050Z",
     "start_time": "2023-06-19T17:20:59.238221Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def generate_labels_from_csv(file_path):\n",
    "    with open(file_path, 'r') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        data = list(reader)\n",
    "\n",
    "    counter = Counter()  # 使用 Counter 对标签进行计数\n",
    "    num_class_dict = {}  # 存储每个类别需要生成的节点数\n",
    "    n = len(data)  # 数据总数\n",
    "\n",
    "    # 统计每个类别的出现次数\n",
    "    for row in data:\n",
    "        label = int(row['label'])\n",
    "        counter[label] += 1\n",
    "\n",
    "    # 根据标签出现次数进行排序\n",
    "    sorted_counter = sorted(counter.items(), key=lambda x: x[1])\n",
    "    sum_ = 0\n",
    "    labels_syn = []  # 合成标签列表\n",
    "    syn_class_indices = {}  # 记录每个类别的索引范围\n",
    "\n",
    "    # 计算每个类别需要生成的节点数，并生成合成标签\n",
    "    for ix, (c, num) in enumerate(sorted_counter):\n",
    "        if ix == len(sorted_counter) - 1:\n",
    "            num_class_dict[c] = int(n * 0.05) - sum_  \n",
    "            # 如果是最后一个类别，那么直接计算需要生成的节点数，并将这个类别的合成标签添加到 labels_syn 列表中\n",
    "            syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "            labels_syn += [c] * num_class_dict[c]\n",
    "        else:\n",
    "            num_class_dict[c] = max(int(num * 0.05), 1)  # 其他类别的节点数\n",
    "            # 如果不是最后一个类别，那么先计算需要生成的节点数，然后将这个类别的合成标签添加到 labels_syn 列表中。\n",
    "            sum_ += num_class_dict[c]\n",
    "            syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "            labels_syn += [c] * num_class_dict[c]\n",
    "\n",
    "    return n * 0.05,labels_syn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd691942",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:20:59.270331Z",
     "start_time": "2023-06-19T17:20:59.261038Z"
    }
   },
   "outputs": [],
   "source": [
    "def loadCSV():\n",
    "\n",
    "    dictLabels = {}\n",
    "\n",
    "    # dictLabels（标签到子图的映射）、\n",
    "\n",
    "    with open(r\"D:\\pythonProject\\python_file\\Graph_DD\\META-DD\\DATA\\arxiv\\train.csv\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        next(csvreader, None)  # skip (filename, label)\n",
    "        for i, row in enumerate(csvreader):\n",
    "            filename = row[1]\n",
    "            g_idx = int(filename.split('_')[0])\n",
    "            label = row[2]\n",
    "            # append filename to current label\n",
    "\n",
    "            if label in dictLabels.keys():\n",
    "                dictLabels[label].append(filename)\n",
    "            else:\n",
    "                dictLabels[label] = [filename]\n",
    "\n",
    "    return dictLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f09b10a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:20:59.283535Z",
     "start_time": "2023-06-19T17:20:59.272699Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_sub_adj_feat(features, labels_syn, label_dict):\n",
    "    idx_selected = []\n",
    "\n",
    "    from collections import Counter\n",
    "    counter = Counter(labels_syn.cpu().numpy())\n",
    "    selected_features = []\n",
    "    for c in counter.keys():\n",
    "        class_label = str(c)\n",
    "        if class_label in label_dict:\n",
    "            node_indices = [label_dict[class_label].index(node) for node in label_dict[class_label]]\n",
    "            num_features = counter[c]\n",
    "            selected_indices = np.random.choice(node_indices, size=num_features, replace=False)\n",
    "            idx_selected.extend(selected_indices)\n",
    "\n",
    "            selected_features.extend(features[selected_indices])\n",
    "\n",
    "    selected_features = np.array(selected_features)\n",
    "\n",
    "    return selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "187469ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:20:59.364675Z",
     "start_time": "2023-06-19T17:20:59.283535Z"
    }
   },
   "outputs": [],
   "source": [
    "feat = np.load( 'D:/pythonProject/python_file/Graph_DD/G-Meta-master/DATA/arxiv/features.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "609e808b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:21:16.005869Z",
     "start_time": "2023-06-19T17:20:59.364675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj_syn: (5034, 5034) feat_syn: torch.Size([5034, 128])\n"
     ]
    }
   ],
   "source": [
    "n,labels_syn = generate_labels_from_csv('D:/pythonProject/python_file/Graph_DD/G-Meta-master/DATA/arxiv/train.csv')\n",
    "# 计算需要生成的synthetic graph的节点数\n",
    "n = int(n)\n",
    "# 获取特征维度\n",
    "d = 128\n",
    "device='cuda'\n",
    "# 初始化synthetic graph的节点数和特征\n",
    "nnodes_syn = n\n",
    "\n",
    "\n",
    "\n",
    "# 初始化PGE模型\n",
    "pge = PGE(nfeat=d, nnodes=n, device=device, args=args).to(device)\n",
    "\n",
    "# 生成synthetic graph的标签\n",
    "labels_syn = torch.LongTensor(labels_syn).to(device)\n",
    "\n",
    "dictLabels = loadCSV()\n",
    "feat_syn = torch.from_numpy(get_sub_adj_feat(feat, labels_syn, dictLabels)).to(device)\n",
    "# # 重置synthetic graph的特征\n",
    "# reset_parameters()\n",
    "\n",
    "# 初始化特征优化器和PGE模型优化器\n",
    "optimizer_feat = torch.optim.Adam([feat_syn], lr=0.01)\n",
    "optimizer_pge = torch.optim.Adam(pge.parameters(), lr=0.01)\n",
    "\n",
    "# 打印synthetic graph的节点数和特征维度\n",
    "print('adj_syn:', (n,n), 'feat_syn:', feat_syn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42b75437",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:21:19.230425Z",
     "start_time": "2023-06-19T17:21:16.009543Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.08 GiB (GPU 0; 6.00 GiB total capacity; 196.69 MiB already allocated; 4.79 GiB free; 216.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m adj_syn \u001b[38;5;241m=\u001b[39m \u001b[43mpge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_syn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\pythonProject\\python_file\\Graph_DD\\META-DD\\G-Meta\\models\\parametrized_adj.py:60\u001b[0m, in \u001b[0;36mPGE.forward\u001b[1;34m(self, x, inference)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     edge_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_index\n\u001b[1;32m---> 60\u001b[0m     edge_embed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     61\u001b[0m             x[edge_index[\u001b[38;5;241m1\u001b[39m]]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ix, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m     63\u001b[0m         edge_embed \u001b[38;5;241m=\u001b[39m layer(edge_embed)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.08 GiB (GPU 0; 6.00 GiB total capacity; 196.69 MiB already allocated; 4.79 GiB free; 216.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "adj_syn = pge(feat_syn)\n",
    "# adj_syn_norm = utils.normalize_adj_tensor(adj_syn, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c0494",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:21:19.234548Z",
     "start_time": "2023-06-19T17:21:19.234548Z"
    }
   },
   "outputs": [],
   "source": [
    "adj_syn[adj_syn < 0.5] = 0\n",
    "adj_syn[adj_syn >= 0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ec49d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:21:19.235546Z",
     "start_time": "2023-06-19T17:21:19.235546Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "adj_syn_norm_sp = sp.csr_matrix(adj_syn.cpu().detach().numpy())\n",
    "g = dgl.from_scipy(adj_syn_norm_sp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae416e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:21:19.236560Z",
     "start_time": "2023-06-19T17:21:19.236560Z"
    }
   },
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5c397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:21:19.237546Z",
     "start_time": "2023-06-19T17:21:19.237546Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将labels_syn转换为dictLabels形式的字典\n",
    "dict_labels = {f\"0_{i}\": labels_syn[i].item() for i in range(len(labels_syn))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66baf88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:21:19.239622Z",
     "start_time": "2023-06-19T17:21:19.239622Z"
    }
   },
   "outputs": [],
   "source": [
    "db_val = Subgraphs_syn('syn', dict_labels, n_way=args.n_way, k_shot=args.k_spt,k_query=args.k_qry, batchsz=100, args = args, adjs = g, h = args.h, labels = labels_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07309a8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:21:19.239622Z",
     "start_time": "2023-06-19T17:21:19.239622Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "        graphs_spt, labels_spt, graph_qry, labels_qry, center_spt, center_qry, nodeidx_spt, nodeidx_qry, support_graph_idx, query_graph_idx = map(list, zip(*samples))\n",
    "\n",
    "        return graphs_spt, labels_spt, graph_qry, labels_qry, center_spt, center_qry, nodeidx_spt, nodeidx_qry, support_graph_idx, query_graph_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835de356",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:21:19.239622Z",
     "start_time": "2023-06-19T17:21:19.239622Z"
    }
   },
   "outputs": [],
   "source": [
    "db = DataLoader(db_val, args.task_num, shuffle=True, num_workers=args.num_workers, pin_memory=True, collate_fn = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8b6dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:21:19.243491Z",
     "start_time": "2023-06-19T17:21:19.243491Z"
    }
   },
   "outputs": [],
   "source": [
    "num_steps = len(db)\n",
    "print(\"Number of steps:\", num_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ee3e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:21:19.245223Z",
     "start_time": "2023-06-19T17:21:19.245223Z"
    }
   },
   "outputs": [],
   "source": [
    "for step, (x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry) in enumerate(db):\n",
    "        print(\"Batch\", step)\n",
    "        print(\"------------------------\")\n",
    "        print(\"x_spt:\", x_spt)\n",
    "        print(\"------------------------\")\n",
    "        print(\"y_spt:\", y_spt)\n",
    "        print(\"------------------------\")\n",
    "        print(\"x_qry:\", x_qry)\n",
    "        print(\"------------------------\")\n",
    "        print(\"y_qry:\", y_qry)\n",
    "        print(\"------------------------\")\n",
    "        print(\"c_spt:\", c_spt)\n",
    "        print(\"------------------------\")\n",
    "        print(\"c_qry:\", c_qry)\n",
    "        print(\"------------------------\")\n",
    "        print(\"n_spt:\", n_spt)\n",
    "        print(\"------------------------\")\n",
    "        print(\"n_qry:\", n_qry)\n",
    "        print(\"------------------------\")\n",
    "        print(\"g_spt:\", g_spt)\n",
    "        print(\"------------------------\")\n",
    "        print(\"g_qry:\", g_qry)\n",
    "        print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c9cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8a6fc8bb",
   "metadata": {},
   "source": [
    "# 生成的数据全都是训练集，训练完以后用原始数据的测试集测试效果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
