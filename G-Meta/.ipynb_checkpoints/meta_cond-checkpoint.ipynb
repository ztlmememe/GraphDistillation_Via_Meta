{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54156c",
   "metadata": {},
   "outputs": [],
   "source": [
    " def collate(samples):\n",
    "    graphs_spt, labels_spt, graph_qry, labels_qry, center_spt, center_qry, nodeidx_spt, nodeidx_qry, support_graph_idx, query_graph_idx = map(list, zip(*samples))\n",
    "\n",
    "    return graphs_spt, labels_spt, graph_qry, labels_qry, center_spt, center_qry, nodeidx_spt, nodeidx_qry, support_graph_idx, query_graph_idx\n",
    "\n",
    "class GCond:\n",
    "    # 初始化函数，接收数据、参数和设备信息\n",
    "    def __init__(self, data, args, device='cuda', **kwargs):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "\n",
    "        # 计算需要生成的synthetic graph的节点数\n",
    "        n = int(len(data.idx_train) * args.reduction_rate)\n",
    "\n",
    "        # 获取特征维度\n",
    "        d = data.feat_train.shape[1]\n",
    "\n",
    "        # 初始化synthetic graph的节点数和特征\n",
    "        self.nnodes_syn = n\n",
    "        self.feat_syn = nn.Parameter(torch.FloatTensor(n, d).to(device))\n",
    "\n",
    "        # 初始化PGE模型\n",
    "        self.pge = PGE(nfeat=d, nnodes=n, device=device, args=args).to(device)\n",
    "\n",
    "        # 生成synthetic graph的标签\n",
    "        self.labels_syn = torch.LongTensor(self.generate_labels_syn(data)).to(device)\n",
    "        \n",
    "        # 重置synthetic graph的特征\n",
    "        self.reset_parameters()\n",
    "\n",
    "        # 初始化特征优化器和PGE模型优化器\n",
    "        self.optimizer_feat = torch.optim.Adam([self.feat_syn], lr=args.lr_feat)\n",
    "        self.optimizer_pge = torch.optim.Adam(self.pge.parameters(), lr=args.lr_adj)\n",
    "        \n",
    "        # 打印synthetic graph的节点数和特征维度\n",
    "        print('adj_syn:', (n,n), 'feat_syn:', self.feat_syn.shape)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.feat_syn.data.copy_(torch.randn(self.feat_syn.size()))\n",
    "\n",
    "    def loadCSV(self,path):\n",
    "        import csv\n",
    "        from collections import Counter\n",
    "        dictLabels = {}\n",
    "\n",
    "        # dictLabels（标签到子图的映射）、\n",
    "\n",
    "        with open(path, 'r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter=',')\n",
    "            next(csvreader, None)  # skip (filename, label)\n",
    "            for i, row in enumerate(csvreader):\n",
    "                filename = row[1]\n",
    "                g_idx = int(filename.split('_')[0])\n",
    "                label = row[2]\n",
    "                # append filename to current label\n",
    "\n",
    "                if label in dictLabels.keys():\n",
    "                    dictLabels[label].append(filename)\n",
    "                else:\n",
    "                    dictLabels[label] = [filename]\n",
    "\n",
    "        return dictLabels\n",
    "\n",
    "    def generate_labels_from_csv(self,file_path,ratio):\n",
    "        import csv\n",
    "        from collections import Counter\n",
    "        with open(file_path, 'r') as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "            data = list(reader)\n",
    "\n",
    "        counter = Counter()  # 使用 Counter 对标签进行计数\n",
    "        num_class_dict = {}  # 存储每个类别需要生成的节点数\n",
    "        n = len(data)  # 数据总数\n",
    "\n",
    "        # 统计每个类别的出现次数\n",
    "        for row in data:\n",
    "            label = int(row['label'])\n",
    "            counter[label] += 1\n",
    "\n",
    "        # 根据标签出现次数进行排序\n",
    "        sorted_counter = sorted(counter.items(), key=lambda x: x[1])\n",
    "        sum_ = 0\n",
    "        labels_syn = []  # 合成标签列表\n",
    "        syn_class_indices = {}  # 记录每个类别的索引范围\n",
    "    #     ratio = 0.08\n",
    "        # 计算每个类别需要生成的节点数，并生成合成标签\n",
    "        for ix, (c, num) in enumerate(sorted_counter):\n",
    "\n",
    "            if ix == len(sorted_counter) - 1:\n",
    "                num_class_dict[c] = int(n * ratio) - sum_  \n",
    "                # 如果是最后一个类别，那么直接计算需要生成的节点数，并将这个类别的合成标签添加到 labels_syn 列表中\n",
    "                syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "                labels_syn += [c] * num_class_dict[c]\n",
    "\n",
    "            else:\n",
    "                num_class_dict[c] = max(int(num * ratio), 1)  # 其他类别的节点数\n",
    "                # 如果不是最后一个类别，那么先计算需要生成的节点数，然后将这个类别的合成标签添加到 labels_syn 列表中。\n",
    "                sum_ += num_class_dict[c]\n",
    "                syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "                labels_syn += [c] * num_class_dict[c]\n",
    "\n",
    "        return n * ratio,labels_syn\n",
    "\n",
    "    def get_sub_adj_feat(self,features, labels_syn, label_dict):\n",
    "        idx_selected = []\n",
    "\n",
    "        from collections import Counter\n",
    "        counter = Counter(labels_syn.cpu().numpy())\n",
    "        selected_features = []\n",
    "        for c in counter.keys():\n",
    "            class_label = str(c)\n",
    "            if class_label in label_dict:\n",
    "                node_indices = [label_dict[class_label].index(node) for node in label_dict[class_label]]\n",
    "                num_features = counter[c]\n",
    "                selected_indices = np.random.choice(node_indices, size=num_features, replace=False)\n",
    "                idx_selected.extend(selected_indices)\n",
    "\n",
    "                selected_features.extend(features[selected_indices])\n",
    "\n",
    "        selected_features = np.array(selected_features)\n",
    "\n",
    "        return selected_features\n",
    "    \n",
    "    def generate_syn_feat(self,feat,pge = None,labels_syn,dictLabels,args):\n",
    "        path = \"D:\\pythonProject\\python_file\\Graph_DD\\META-DD\\DATA\\arxiv\\train.csv\"\n",
    "        \n",
    "        if pge == None:\n",
    "            n,labels_syn= generate_labels_from_csv(path)\n",
    "\n",
    "            # 计算需要生成的synthetic graph的节点数\n",
    "            n = int(n)\n",
    "            # 获取特征维度\n",
    "            d = 128\n",
    "            device='cuda'\n",
    "            # 初始化synthetic graph的节点数和特征\n",
    "            nnodes_syn = n\n",
    "            # 初始化PGE模型\n",
    "            pge = PGE(nfeat=d, nnodes=n, device=device, args=args).to(device)\n",
    "    \n",
    "        # 生成synthetic graph的标签\n",
    "        labels_syn = torch.LongTensor(labels_syn).to(device)\n",
    "\n",
    "#         dictLabels = loadCSV(path)\n",
    "        feat_syn = torch.from_numpy(get_sub_adj_feat(feat, labels_syn, dictLabels)).to(device)\n",
    "    # # 重置synthetic graph的特征\n",
    "    # reset_parameters()\n",
    "        adj_syn = pge(feat_syn)\n",
    "        adj_syn[adj_syn < 0.5] = 0\n",
    "        adj_syn[adj_syn >= 0.5] = 1\n",
    "\n",
    "        adj_syn_norm_sp = sp.csr_matrix(adj_syn.cpu().detach().numpy())\n",
    "        g = dgl.from_scipy(adj_syn_norm_sp) # 生成的合成图太过密集，导致二跳子图约等于全图\n",
    "        # 将labels_syn转换为dictLabels形式的字典\n",
    "        dict_labels = {f\"0_{i}\": labels_syn[i].item() for i in range(len(labels_syn))}\n",
    "        db_syn = Subgraphs_syn('syn', dict_labels, n_way=args.n_way, k_shot=args.k_spt,k_query=args.k_qry, batchsz=100, args = args, adjs = g, h = args.h, labels = labels_syn)\n",
    "       \n",
    "\n",
    "        db = DataLoader(db_syn, args.task_num, shuffle=True, num_workers=args.num_workers, pin_memory=True, collate_fn = collate)\n",
    "        \n",
    "        return db\n",
    "    def train(self, verbose=True):\n",
    "        # 获取参数和数据\n",
    "        args = self.args\n",
    "        data = self.data\n",
    "        feat_syn, pge, labels_syn = self.feat_syn, self.pge, self.labels_syn\n",
    "        features, adj, labels = data.feat_train, data.adj_train, data.labels_train\n",
    "        syn_class_indices = self.syn_class_indices\n",
    "\n",
    "        # 将数据转换为张量并将其放置在设备上\n",
    "        features, adj, labels = utils.to_tensor(features, adj, labels, device=self.device)\n",
    "        # 获取子邻接矩阵和子特征矩阵\n",
    "        feat_sub, adj_sub = self.get_sub_adj_feat(features)\n",
    "        # 将子特征矩阵复制到feat_syn中\n",
    "        self.feat_syn.data.copy_(feat_sub)\n",
    "\n",
    "        # 根据是否是稀疏张量来规范化邻接矩阵\n",
    "        if utils.is_sparse_tensor(adj):\n",
    "            adj_norm = utils.normalize_adj_tensor(adj, sparse=True)\n",
    "        else:\n",
    "            adj_norm = utils.normalize_adj_tensor(adj)\n",
    "\n",
    "        # 将邻接矩阵转换为稀疏张量\n",
    "        adj = adj_norm\n",
    "        adj = SparseTensor(row=adj._indices()[0], col=adj._indices()[1],\n",
    "                value=adj._values(), sparse_sizes=adj.size()).t()\n",
    "\n",
    "        # 获取外部循环和内部循环的次数\n",
    "        outer_loop, inner_loop = get_loops(args)\n",
    "\n",
    "        for it in range(args.epochs+1):\n",
    "            loss_avg = 0\n",
    "            if args.sgc==1:\n",
    "                model = SGC(nfeat=data.feat_train.shape[1], nhid=args.hidden,\n",
    "                            nclass=data.nclass, dropout=args.dropout,\n",
    "                            nlayers=args.nlayers, with_bn=False,\n",
    "                            device=self.device).to(self.device)\n",
    "            elif args.sgc==2:\n",
    "                model = SGC1(nfeat=data.feat_train.shape[1], nhid=args.hidden,\n",
    "                            nclass=data.nclass, dropout=args.dropout,\n",
    "                            nlayers=args.nlayers, with_bn=False,\n",
    "                            device=self.device).to(self.device)\n",
    "\n",
    "            else:\n",
    "                model = GCN(nfeat=data.feat_train.shape[1], nhid=args.hidden,\n",
    "                            nclass=data.nclass, dropout=args.dropout, nlayers=args.nlayers,\n",
    "                            device=self.device).to(self.device)\n",
    "\n",
    "            model.initialize() # model是GNN模型\n",
    "\n",
    "            model_parameters = list(model.parameters())\n",
    "\n",
    "            optimizer_model = torch.optim.Adam(model_parameters, lr=args.lr_model)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            # 进行外部循环\n",
    "            for ol in range(outer_loop):\n",
    "                # 生成并规范化合成邻接矩阵\n",
    "                adj_syn = pge(self.feat_syn)\n",
    "                adj_syn_norm = utils.normalize_adj_tensor(adj_syn, sparse=False)\n",
    "                feat_syn_norm = feat_syn\n",
    "\n",
    "                # 判断是否有BatchNorm层\n",
    "                BN_flag = False\n",
    "                for module in model.modules():\n",
    "                    if 'BatchNorm' in module._get_name(): #BatchNorm\n",
    "                        BN_flag = True\n",
    "\n",
    "                # 如果有BatchNorm层，则需要训练模型以更新BatchNorm层的mu和sigma\n",
    "                if BN_flag:\n",
    "                    model.train() # for updating the mu, sigma of BatchNorm\n",
    "                    output_real = model.forward(features, adj_norm)\n",
    "                    for module in model.modules():\n",
    "                        if 'BatchNorm' in module._get_name():  #BatchNorm\n",
    "                            module.eval() # fix mu and sigma of every BatchNorm layer\n",
    "\n",
    "                loss = torch.tensor(0.0).to(self.device)\n",
    "                for c in range(data.nclass): # 根据每个类别计算LOSS\n",
    "                    if c not in self.num_class_dict:\n",
    "                        continue\n",
    "                    # 获取类别c的样本\n",
    "                    batch_size, n_id, adjs = data.retrieve_class_sampler(\n",
    "                            c, adj, transductive=False, args=args)\n",
    "\n",
    "                    # 如果只有一层，则将邻接矩阵放入列表中\n",
    "                    if args.nlayers == 1:\n",
    "                        adjs = [adjs]\n",
    "                    adjs = [adj.to(self.device) for adj in adjs]\n",
    "\n",
    "                    # 根据采样结果进行前向传播，LOSS计算和梯度计算\n",
    "                    output = model.forward_sampler(features[n_id], adjs)\n",
    "                    loss_real = F.nll_loss(output, labels[n_id[:batch_size]])\n",
    "                    gw_real = torch.autograd.grad(loss_real, model_parameters)\n",
    "                    gw_real = list((_.detach().clone() for _ in gw_real))\n",
    "\n",
    "                    # 获取类别c的合成邻接矩阵\n",
    "                    ind = syn_class_indices[c]\n",
    "                    if args.nlayers == 1:\n",
    "                        adj_syn_norm_list = [adj_syn_norm[ind[0]: ind[1]]]\n",
    "                    else:\n",
    "                        adj_syn_norm_list = [adj_syn_norm]*(args.nlayers-1) + \\\n",
    "                                [adj_syn_norm[ind[0]: ind[1]]]\n",
    "\n",
    "                    # 计算合成邻接矩阵下的输出和LOSS\n",
    "                    output_syn = model.forward_sampler_syn(feat_syn, adj_syn_norm_list)\n",
    "                    loss_syn = F.nll_loss(output_syn, labels_syn[ind[0]: ind[1]])\n",
    "\n",
    "                    # 计算合成邻接矩阵下的梯度\n",
    "                    gw_syn = torch.autograd.grad(loss_syn, model_parameters, create_graph=True)\n",
    "                    # create_graph：一个布尔值，表示是否创建用于计算高阶导数的计算图。默认为False，表示只计算一阶导数。\n",
    "                   \n",
    "                    # 计算匹配损失\n",
    "                    coeff = self.num_class_dict[c] / max(self.num_class_dict.values())\n",
    "                    loss += coeff  * match_loss(gw_syn, gw_real, args, device=self.device)\n",
    "\n",
    "                loss_avg += loss.item()\n",
    "                # 计算正则化损失\n",
    "                # TODO: regularize\n",
    "                if args.alpha > 0:\n",
    "                    loss_reg = args.alpha * regularization(adj_syn, utils.tensor2onehot(labels_syn))\n",
    "                # else:\n",
    "                else:\n",
    "                    loss_reg = torch.tensor(0)\n",
    "\n",
    "                loss = loss + loss_reg\n",
    "\n",
    "                # 更新合成图\n",
    "                self.optimizer_feat.zero_grad()\n",
    "                # 每次计算梯度时，梯度都会被累加到梯度缓存中。\n",
    "                # 因此，在每次更新模型参数之前需要将梯度缓存清零，以避免梯度累加的影响。\n",
    "                self.optimizer_pge.zero_grad()\n",
    "                loss.backward() # 计算损失函数对于模型参数的梯度\n",
    "\n",
    "                # 根据 it 的值选择更新 self.optimizer_pge 或 self.optimizer_feat\n",
    "                if it % 50 < 10:\n",
    "                    self.optimizer_pge.step() # 使用优化算法来更新模型参数\n",
    "                else:\n",
    "                    self.optimizer_feat.step()\n",
    "\n",
    "                if args.debug and ol % 5 ==0: # 打印梯度匹配损失\n",
    "                    print('Gradient matching loss:', loss.item())\n",
    "\n",
    "                if ol == outer_loop - 1:\n",
    "                    # print('loss_reg:', loss_reg.item())\n",
    "                    # print('Gradient matching loss:', loss.item())\n",
    "                    break\n",
    "\n",
    "                # 进行内循环，更新 GNN 模型的参数\n",
    "                feat_syn_inner = feat_syn.detach()\n",
    "                adj_syn_inner = pge.inference(feat_syn)\n",
    "                adj_syn_inner_norm = utils.normalize_adj_tensor(adj_syn_inner, sparse=False)\n",
    "                feat_syn_inner_norm = feat_syn_inner\n",
    "                for j in range(inner_loop):\n",
    "                    optimizer_model.zero_grad()\n",
    "                    output_syn_inner = model.forward(feat_syn_inner_norm, adj_syn_inner_norm)\n",
    "                    loss_syn_inner = F.nll_loss(output_syn_inner, labels_syn)\n",
    "                    loss_syn_inner.backward()\n",
    "                    optimizer_model.step() # update gnn param\n",
    "\n",
    "            # 计算平均损失并打印\n",
    "            loss_avg /= (data.nclass*outer_loop)\n",
    "            if it % 50 == 0:\n",
    "                print('Epoch {}, loss_avg: {}'.format(it, loss_avg))\n",
    "\n",
    "            eval_epochs = [100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 3000, 4000, 5000]\n",
    "\n",
    "            if verbose and it in eval_epochs:\n",
    "            # if verbose and (it+1) % 500 == 0:\n",
    "                res = []\n",
    "                runs = 1 if args.dataset in ['ogbn-arxiv', 'reddit', 'flickr'] else 3\n",
    "                for i in range(runs):\n",
    "                    # self.test()\n",
    "                    res.append(self.test_with_val())\n",
    "                res = np.array(res)\n",
    "                print('Test:',\n",
    "                        repr([res.mean(0), res.std(0)]))\n",
    "\n",
    "\n",
    "\n",
    "    def get_sub_adj_feat(self, features):\n",
    "        data = self.data\n",
    "        args = self.args\n",
    "        idx_selected = []\n",
    "\n",
    "        from collections import Counter;\n",
    "        # 计算了 self.labels_syn.cpu().numpy() 中每个元素的出现次数\n",
    "        counter = Counter(self.labels_syn.cpu().numpy())\n",
    "\n",
    "        for c in range(data.nclass):\n",
    "            tmp = data.retrieve_class(c, num=counter[c])\n",
    "            tmp = list(tmp)\n",
    "            idx_selected = idx_selected + tmp\n",
    "        idx_selected = np.array(idx_selected).reshape(-1)\n",
    "        features = features[idx_selected]\n",
    "\n",
    "        # adj_knn = torch.zeros((data.nclass*args.nsamples, data.nclass*args.nsamples)).to(self.device)\n",
    "        # for i in range(data.nclass):\n",
    "        #     idx = np.arange(i*args.nsamples, i*args.nsamples+args.nsamples)\n",
    "        #     adj_knn[np.ix_(idx, idx)] = 1\n",
    "\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        # features[features!=0] = 1\n",
    "        k = 2\n",
    "        sims = cosine_similarity(features.cpu().numpy())\n",
    "        sims[(np.arange(len(sims)), np.arange(len(sims)))] = 0\n",
    "        for i in range(len(sims)):\n",
    "            indices_argsort = np.argsort(sims[i])\n",
    "            sims[i, indices_argsort[: -k]] = 0\n",
    "        adj_knn = torch.FloatTensor(sims).to(self.device)\n",
    "        return features, adj_knn\n",
    "\n",
    "\n",
    "def get_loops(args):\n",
    "    # Get the two hyper-parameters of outer-loop and inner-loop.\n",
    "    # The following values are empirically good.\n",
    "    if args.one_step:\n",
    "        return 10, 0\n",
    "\n",
    "    if args.dataset in ['ogbn-arxiv']:\n",
    "        return 20, 0\n",
    "    if args.dataset in ['reddit']:\n",
    "        return args.outer, args.inner\n",
    "    if args.dataset in ['flickr']:\n",
    "        return args.outer, args.inner\n",
    "        # return 10, 1\n",
    "    if args.dataset in ['cora']:\n",
    "        return 20, 10\n",
    "    if args.dataset in ['citeseer']:\n",
    "        return 20, 5 # at least 200 epochs\n",
    "    else:\n",
    "        return 20, 5\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
