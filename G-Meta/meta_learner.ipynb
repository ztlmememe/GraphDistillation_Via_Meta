{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f486f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    graphs_spt, labels_spt, graph_qry, labels_qry, center_spt, center_qry, nodeidx_spt, nodeidx_qry, support_graph_idx, query_graph_idx = map(list, zip(*samples))\n",
    "\n",
    "    return graphs_spt, labels_spt, graph_qry, labels_qry, center_spt, center_qry, nodeidx_spt, nodeidx_qry, support_graph_idx, query_graph_idx\n",
    "\n",
    "class Meta_learner():\n",
    "    # 初始化函数，接收数据、参数和设备信息\n",
    "    def __init__(self, db_train,db_val,db_test, args, device='cuda', **kwargs):\n",
    "        self.db_train = db_train\n",
    "        self.db_val = db_val\n",
    "        self.db_test = db_test\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        path = r\"..\\DATA\\arxiv\\train.csv\"\n",
    "        \n",
    "        ratio = 0.0025 # 0.25%\n",
    "        n,labels_syn= self.generate_labels_from_csv(path,ratio)\n",
    "\n",
    "        # 计算需要生成的synthetic graph的节点数\n",
    "        n = int(n)        \n",
    "        # 初始化synthetic graph的节点数和特征\n",
    "        self.nnodes_syn = n\n",
    "        # 获取特征维度\n",
    "        self.d = 128\n",
    "        self.feat_syn = nn.Parameter(torch.FloatTensor(n, d).to(device))\n",
    "\n",
    "        # 初始化PGE模型\n",
    "        self.pge = PGE(nfeat=d, nnodes=n, device=self.device, args=self.args).to(device)\n",
    "        \n",
    "        # 生成synthetic graph的标签\n",
    "        self.labels_syn = torch.LongTensor(labels_syn).to(device)\n",
    "\n",
    "        config = [('GraphConv', [self.feat[0].shape[1], self.args.hidden_dim])]\n",
    "\n",
    "        if self.args.h > 1:\n",
    "            config = config + [('GraphConv', [self.args.hidden_dim, self.args.hidden_dim])] * (self.args.h - 1)\n",
    "\n",
    "        config = config + [('Linear', [self.args.hidden_dim, self.labels_num])]\n",
    "        self.maml = Meta(args, config).to(device)\n",
    "        # 重置synthetic graph的特征\n",
    "        self.reset_parameters()\n",
    "\n",
    "        self.dictLabels = self.loadCSV(path)\n",
    "        # 初始化特征优化器和PGE模型优化器\n",
    "        self.optimizer_feat = torch.optim.Adam([self.feat_syn], lr=args.lr_feat)\n",
    "        self.optimizer_pge = torch.optim.Adam(self.pge.parameters(), lr=args.lr_adj)\n",
    "        \n",
    "        # 打印synthetic graph的节点数和特征维度\n",
    "        print('adj_syn:', (n,n), 'feat_syn:', self.feat_syn.shape)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.feat_syn.data.copy_(torch.randn(self.feat_syn.size()))\n",
    "\n",
    "    def loadCSV(self,path):\n",
    "        import csv\n",
    "        from collections import Counter\n",
    "        dictLabels = {}\n",
    "\n",
    "        # dictLabels（标签到子图的映射）、\n",
    "\n",
    "        with open(path, 'r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter=',')\n",
    "            next(csvreader, None)  # skip (filename, label)\n",
    "            for i, row in enumerate(csvreader):\n",
    "                filename = row[1]\n",
    "                g_idx = int(filename.split('_')[0])\n",
    "                label = row[2]\n",
    "                # append filename to current label\n",
    "\n",
    "                if label in dictLabels.keys():\n",
    "                    dictLabels[label].append(filename)\n",
    "                else:\n",
    "                    dictLabels[label] = [filename]\n",
    "\n",
    "        return dictLabels\n",
    "\n",
    "    def generate_labels_from_csv(self,file_path,ratio):\n",
    "        import csv\n",
    "        from collections import Counter\n",
    "        with open(file_path, 'r') as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "            data = list(reader)\n",
    "\n",
    "        counter = Counter()  # 使用 Counter 对标签进行计数\n",
    "        num_class_dict = {}  # 存储每个类别需要生成的节点数\n",
    "        n = len(data)  # 数据总数\n",
    "\n",
    "        # 统计每个类别的出现次数\n",
    "        for row in data:\n",
    "            label = int(row['label'])\n",
    "            counter[label] += 1\n",
    "\n",
    "        # 根据标签出现次数进行排序\n",
    "        sorted_counter = sorted(counter.items(), key=lambda x: x[1])\n",
    "        sum_ = 0\n",
    "        labels_syn = []  # 合成标签列表\n",
    "        syn_class_indices = {}  # 记录每个类别的索引范围\n",
    "    #     ratio = 0.08\n",
    "        # 计算每个类别需要生成的节点数，并生成合成标签\n",
    "        for ix, (c, num) in enumerate(sorted_counter):\n",
    "\n",
    "            if ix == len(sorted_counter) - 1:\n",
    "                num_class_dict[c] = int(n * ratio) - sum_  \n",
    "                # 如果是最后一个类别，那么直接计算需要生成的节点数，并将这个类别的合成标签添加到 labels_syn 列表中\n",
    "                syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "                labels_syn += [c] * num_class_dict[c]\n",
    "\n",
    "            else:\n",
    "                num_class_dict[c] = max(int(num * ratio), 1)  # 其他类别的节点数\n",
    "                # 如果不是最后一个类别，那么先计算需要生成的节点数，然后将这个类别的合成标签添加到 labels_syn 列表中。\n",
    "                sum_ += num_class_dict[c]\n",
    "                syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "                labels_syn += [c] * num_class_dict[c]\n",
    "\n",
    "        return n * ratio,labels_syn\n",
    "\n",
    "    def get_sub_adj_feat(self,features, labels_syn, label_dict):\n",
    "        idx_selected = []\n",
    "\n",
    "        from collections import Counter\n",
    "        counter = Counter(labels_syn.cpu().numpy())\n",
    "        selected_features = []\n",
    "        for c in counter.keys():\n",
    "            class_label = str(c)\n",
    "            if class_label in label_dict:\n",
    "                node_indices = [label_dict[class_label].index(node) for node in label_dict[class_label]]\n",
    "                num_features = counter[c]\n",
    "                selected_indices = np.random.choice(node_indices, size=num_features, replace=False)\n",
    "                idx_selected.extend(selected_indices)\n",
    "\n",
    "                selected_features.extend(features[selected_indices])\n",
    "\n",
    "        selected_features = np.array(selected_features)\n",
    "\n",
    "        return selected_features\n",
    "    \n",
    "    def generate_syn_feat_db(self):\n",
    "\n",
    "#         dictLabels = loadCSV(path)\n",
    "        self.feat_syn = torch.from_numpy(self.get_sub_adj_feat(self.feat_syn, self.labels_syn, dictLabels)).to(self.device)\n",
    "    # # 重置synthetic graph的特征\n",
    "    # reset_parameters()\n",
    "        adj_syn_orj = self.pge(self.feat_syn)\n",
    "        adj_syn_norm = utils.normalize_adj_tensor(adj_syn_orj, sparse=False)\n",
    "\n",
    "        adj_syn = adj_syn_norm\n",
    "        adj_syn[adj_syn < 0.5] = 0\n",
    "        adj_syn[adj_syn >= 0.5] = 1\n",
    "\n",
    "        adj_syn_norm_sp = sp.csr_matrix(adj_syn.cpu().detach().numpy())\n",
    "        g = dgl.from_scipy(adj_syn_norm_sp) # 生成的合成图太过密集，导致二跳子图约等于全图\n",
    "        # 将labels_syn转换为dictLabels形式的字典\n",
    "        dict_labels = {f\"0_{i}\": self.labels_syn[i].item() for i in range(len(self.labels_syn))}\n",
    "        db_syn = Subgraphs_syn('syn', dict_labels, n_way=self.args.n_way, k_shot=self.args.k_spt,k_query=self.args.k_qry, batchsz=100, args = self.args, adjs = g, h = self.args.h, labels = self.labels_syn)\n",
    "       \n",
    "\n",
    "        db = DataLoader(db_syn, self.args.task_num, shuffle=True, num_workers=self.args.num_workers, pin_memory=True, collate_fn = collate)\n",
    "        \n",
    "        return db,adj_syn_orj\n",
    "    \n",
    "    def train(self, verbose=True,feat = None,dgl_graph = None):\n",
    "        # 获取参数和数据\n",
    "        args = self.args\n",
    "        # data = self.data\n",
    "        feat = [feat]  \n",
    "        dgl_graph = [dgl_graph]\n",
    "\n",
    "        db_train = self.db_train\n",
    "        db_val = self.db_val\n",
    "        db_test = self.db_test\n",
    "\n",
    "        model_parameters = list(self.maml.parameters())\n",
    "\n",
    "\n",
    "        for epoch in range(self. args.epoch):\n",
    "            db = DataLoader(db_train, args.task_num, shuffle=True, num_workers=args.num_workers, pin_memory=True, collate_fn = collate)\n",
    "            db_syn,adj_syn  = self.generate_syn_feat_db()\n",
    "            loss = torch.tensor(0.0).to(self.device)\n",
    "            for step, (x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry) in enumerate(db): # 处理每个任务\n",
    "                nodes_len = 0\n",
    "                # x_spt: a list of #task_num tasks, where each task is a mini-batch of k-shot * n_way subgraphs\n",
    "                # y_spt: a list of #task_num lists of labels. Each list is of length k-shot * n_way int.                \n",
    "                nodes_len += sum([sum([len(j) for j in i]) for i in n_spt])\n",
    "                loss_q_real = self.maml.grad_opt(x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry, feat)\n",
    "                gw_real = torch.autograd.grad(loss_q_real, model_parameters)\n",
    "                gw_real = list((_.detach().clone() for _ in gw_real))\n",
    "            \n",
    "\n",
    "            for step, (x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry) in enumerate(db_syn): # 处理每个任务\n",
    "                nodes_len = 0\n",
    "                # x_spt: a list of #task_num tasks, where each task is a mini-batch of k-shot * n_way subgraphs\n",
    "                # y_spt: a list of #task_num lists of labels. Each list is of length k-shot * n_way int.                \n",
    "                nodes_len += sum([sum([len(j) for j in i]) for i in n_spt])\n",
    "                loss_q_syn = self.maml.grad_opt(x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry, feat)\n",
    "                gw_syn  = torch.autograd.grad(loss_q_syn , model_parameters)\n",
    "                gw_syn  = list((_.detach().clone() for _ in gw_syn ))\n",
    "\n",
    "            # 计算匹配损失\n",
    "            \n",
    "            loss += match_loss(gw_syn, gw_real, args, device=self.device)\n",
    "\n",
    "            # 计算正则化损失\n",
    "            # TODO: regularize\n",
    "            if args.alpha > 0:\n",
    "                loss_reg = args.alpha * regularization(adj_syn, utils.tensor2onehot(self.labels_syn))\n",
    "            # else:\n",
    "            else:\n",
    "                loss_reg = torch.tensor(0)\n",
    "\n",
    "            loss = loss + loss_reg\n",
    "\n",
    "            # 更新合成图\n",
    "            self.optimizer_feat.zero_grad()\n",
    "            # 每次计算梯度时，梯度都会被累加到梯度缓存中。\n",
    "            # 因此，在每次更新模型参数之前需要将梯度缓存清零，以避免梯度累加的影响。\n",
    "            self.optimizer_pge.zero_grad()\n",
    "            loss.backward() # 计算损失函数对于模型参数的梯度\n",
    "\n",
    "            # 根据 it 的值选择更新 self.optimizer_pge 或 self.optimizer_feat\n",
    "            if epoch % 2 == 0:\n",
    "                self.optimizer_pge.step() # 使用优化算法来更新模型参数\n",
    "            else:\n",
    "                self.optimizer_feat.step()\n",
    "\n",
    "            if args.debug and epoch % 2 ==0: # 打印梯度匹配损失\n",
    "                print('Gradient matching loss:', loss.item())\n",
    "\n",
    "            # if ol == outer_loop - 1:\n",
    "            #         # print('loss_reg:', loss_reg.item())\n",
    "            #         # print('Gradient matching loss:', loss.item())\n",
    "            #     break\n",
    "                            # 进行内循环，更新 GNN 模型的参数\n",
    "            self.maml.meta_optim.zero_grad()\n",
    "            loss_q_syn.backward()\n",
    "            # # 注册回调函数保存梯度\n",
    "            # self.net.parameters().register_hook(save_gradients)\n",
    "            self.meta_optim.step() # update gnn param\n",
    "\n",
    "            # validation per epoch\n",
    "            db_v = DataLoader(db_val, 1, shuffle=True, num_workers=args.num_workers, pin_memory=True, collate_fn = collate)\n",
    "            accs_all_test = []\n",
    "\n",
    "            for x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry in db_v:\n",
    "\n",
    "                accs = self.maml.finetunning(x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry, feat)\n",
    "                accs_all_test.append(accs)\n",
    "\n",
    "            accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "            print('Epoch:', epoch + 1, ' Val acc:', str(accs[-1])[:5])\n",
    "            if accs[-1] > max_acc:\n",
    "                max_acc = accs[-1]\n",
    "                model_max = copy.deepcopy(self.maml)\n",
    "    \n",
    "        db_t = DataLoader(db_test, 1, shuffle=True, num_workers=args.num_workers, pin_memory=True, collate_fn = collate)\n",
    "        accs_all_test = []\n",
    "\n",
    "        for x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry in db_t:\n",
    "            accs = self.maml.finetunning(x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry, feat)\n",
    "            accs_all_test.append(accs)\n",
    "\n",
    "        accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "        print('Test acc:', str(accs[1])[:5])\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
