{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import  torch, os\n",
    "import  numpy as np\n",
    "from    subgraph_data_processing import Subgraphs,Subgraphs_syn\n",
    "import  pickle\n",
    "import numpy as np\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', type=int, default=0, help='gpu id')\n",
    "parser.add_argument('--dataset', type=str, default='cora')\n",
    "parser.add_argument('--dis_metric', type=str, default='ours')\n",
    "parser.add_argument('--epochs', type=int, default=2000)\n",
    "parser.add_argument('--nlayers', type=int, default=3)\n",
    "parser.add_argument('--hidden', type=int, default=256)\n",
    "parser.add_argument('--lr_adj', type=float, default=0.01)\n",
    "parser.add_argument('--lr_feat', type=float, default=0.01)\n",
    "parser.add_argument('--lr_model', type=float, default=0.01)\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0)\n",
    "parser.add_argument('--dropout', type=float, default=0.0)\n",
    "parser.add_argument('--normalize_features', type=bool, default=True)\n",
    "parser.add_argument('--keep_ratio', type=float, default=1.0)\n",
    "parser.add_argument('--reduction_rate', type=float, default=0.0025)\n",
    "parser.add_argument('--seed', type=int, default=15, help='Random seed.')\n",
    "parser.add_argument('--alpha', type=float, default=0, help='regularization term.')\n",
    "parser.add_argument('--debug', type=int, default=0)\n",
    "parser.add_argument('--sgc', type=int, default=1)\n",
    "parser.add_argument('--inner', type=int, default=0)\n",
    "parser.add_argument('--outer', type=int, default=20)\n",
    "parser.add_argument('--save', type=int, default=0)\n",
    "parser.add_argument('--one_step', type=int, default=0)\n",
    "\n",
    "parser.add_argument('--epoch', type=int, help='epoch number', default=10)\n",
    "parser.add_argument('--n_way', type=int, help='n way', default=3)\n",
    "# parser.add_argument('--k_spt', type=int, help='k shot for support set', default=3)\n",
    "# parser.add_argument('--k_qry', type=int, help='k shot for query set', default=24)\n",
    "parser.add_argument('--k_spt', type=int, help='k shot for support set', default=1)\n",
    "parser.add_argument('--k_qry', type=int, help='k shot for query set', default=3)\n",
    "parser.add_argument('--task_num', type=int, help='meta batch size, namely task num', default=32)\n",
    "\n",
    "parser.add_argument('--meta_lr', type=float, help='meta-level outer learning rate', default=1e-4) # 之前是1e-3，为了处理合成数据集上的梯度消失问题，改成1e-4\n",
    "\n",
    "parser.add_argument('--update_lr', type=float, help='task-level inner update learning rate', default=1e-2)\n",
    "parser.add_argument('--update_step', type=int, help='task-level inner update steps', default=10)\n",
    "parser.add_argument('--update_step_test', type=int, help='update steps for finetunning', default=20)\n",
    "parser.add_argument('--input_dim', type=int, help='input feature dim', default=1)\n",
    "parser.add_argument('--hidden_dim', type=int, help='hidden dim', default=256)\n",
    "parser.add_argument('--attention_size', type=int, help='dim of attention_size', default=32)\n",
    "parser.add_argument(\"--data_dir\", default=\"D:\\pythonProject\\python_file\\Graph_DD\\G-Meta-master\\arxiv\", type=str, required=False, help=\"The input data dir.\")\n",
    "parser.add_argument(\"--no_finetune\", default=True, type=str, required=False, help=\"no finetune mode.\")\n",
    "parser.add_argument(\"--task_setup\", default='Disjoint', type=str, required=False, help=\"Select from Disjoint or Shared Setup. For Disjoint-Label, single/multiple graphs are both considered.\")\n",
    "parser.add_argument(\"--method\", default='G-Meta', type=str, required=False, help=\"Use G-Meta\")\n",
    "parser.add_argument('--task_n', type=int, help='task number', default=1)\n",
    "parser.add_argument(\"--task_mode\", default='False', type=str, required=False, help=\"For Evaluating on Tasks\")\n",
    "parser.add_argument(\"--val_result_report_steps\", default=100, type=int, required=False, help=\"validation report\")\n",
    "parser.add_argument(\"--train_result_report_steps\", default=200, type=int, required=False, help=\"training report\")\n",
    "parser.add_argument(\"--num_workers\", default=0, type=int, required=False, help=\"num of workers\")\n",
    "parser.add_argument(\"--batchsz\", default=10000, type=int, required=False, help=\"batch size\")\n",
    "parser.add_argument(\"--link_pred_mode\", default='False', type=str, required=False, help=\"For Link Prediction\")\n",
    "parser.add_argument(\"--h\", default=2, type=int, required=False, help=\"neighborhood size\")\n",
    "parser.add_argument('--sample_nodes', type=int, help='sample nodes if above this number of nodes', default=300)\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 classes \n",
      "shuffle DB :train, b:10000, 3-way, 1-shot, 3-query, 2-hops\n",
      "shuffle DB :val, b:100, 3-way, 1-shot, 3-query, 2-hops\n",
      "shuffle DB :test, b:100, 3-way, 1-shot, 3-query, 2-hops\n",
      "------ Start Training ------\n"
     ]
    }
   ],
   "source": [
    "root = r\"../DATA/arxiv/\"\n",
    "feat = np.load( '../DATA/arxiv/features.npy', allow_pickle = True)\n",
    "with open('../DATA/arxiv/graph_dgl.pkl', 'rb') as f:\n",
    "    dgl_graph = pickle.load(f)\n",
    "with open('../DATA/arxiv/label.pkl', 'rb') as f:\n",
    "    info = pickle.load(f)\n",
    "total_class = len(np.unique(np.array(list(info.values()))))\n",
    "print('There are {} classes '.format(total_class))\n",
    "\n",
    "feat = [feat]  \n",
    "feat_shape = feat[0].shape[1]\n",
    "\n",
    "\n",
    "dgl_graph = [dgl_graph]\n",
    "\n",
    "\n",
    "db_train = Subgraphs(root, 'train', info, n_way=args.n_way, k_shot=args.k_spt, k_query=args.k_qry, batchsz=args.batchsz, args = args, adjs = dgl_graph, h = args.h)\n",
    "db_val = Subgraphs(root, 'val', info, n_way=args.n_way, k_shot=args.k_spt,k_query=args.k_qry, batchsz=100, args = args, adjs = dgl_graph, h = args.h)\n",
    "db_test = Subgraphs(root, 'test', info, n_way=args.n_way, k_shot=args.k_spt,k_query=args.k_qry, batchsz=100, args = args, adjs = dgl_graph, h = args.h)\n",
    "print('------ Start Training ------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Meta_learner import Meta_learner\n",
    "from  subgraph_data_processing import Subgraphs_syn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import dgl\n",
    "from utils import *\n",
    "import deeprobust.graph.utils as utils\n",
    "import torch.nn as nn\n",
    "from utils import match_loss, regularization, row_normalize_tensor\n",
    "from models.parametrized_adj import PGE\n",
    "from meta import Meta\n",
    "import copy\n",
    "\n",
    "def collate(samples):\n",
    "    graphs_spt, labels_spt, graph_qry, labels_qry, center_spt, center_qry, nodeidx_spt, nodeidx_qry, support_graph_idx, query_graph_idx = map(list, zip(*samples))\n",
    "\n",
    "    return graphs_spt, labels_spt, graph_qry, labels_qry, center_spt, center_qry, nodeidx_spt, nodeidx_qry, support_graph_idx, query_graph_idx\n",
    "\n",
    "class Meta_learner():\n",
    "    # 初始化函数，接收数据、参数和设备信息\n",
    "    def __init__(self, db_train,db_val,db_test, args, device, feat_shape= None,**kwargs):\n",
    "        self.db_train = db_train\n",
    "        self.db_val = db_val\n",
    "        self.db_test = db_test\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        \n",
    "        path = r\"../DATA/arxiv/train.csv\"\n",
    "        \n",
    "        ratio = 0.0025 # 0.25%\n",
    "        n,labels_syn= self.generate_labels_from_csv(path,ratio)\n",
    "        self.config = [('GraphConv', [feat_shape, self.args.hidden_dim])]\n",
    "        \n",
    "        if self.args.h > 1:\n",
    "            self.config  = self.config  + [('GraphConv', [self.args.hidden_dim, self.args.hidden_dim])] * (self.args.h - 1)\n",
    "\n",
    "        self.config  = self.config  + [('Linear', [self.args.hidden_dim, args.n_way])]\n",
    "        self.maml = Meta(args, self.config,self.device).to(device)\n",
    "        # 计算需要生成的synthetic graph的节点数\n",
    "        n = int(n)        \n",
    "        # 初始化synthetic graph的节点数和特征\n",
    "        # 获取特征维度\n",
    "        d = 128\n",
    "        self.feat_syn = nn.Parameter(torch.FloatTensor(n, d).to(device))\n",
    "\n",
    "        # 初始化PGE模型\n",
    "        self.pge = PGE(nfeat=d, nnodes=n, device=self.device, args=self.args).to(device)\n",
    "        \n",
    "        # 生成synthetic graph的标签\n",
    "        self.labels_syn = torch.LongTensor(labels_syn).to(device)\n",
    "\n",
    "        self.dictLabels = self.loadCSV(path)\n",
    "        # 初始化特征优化器和PGE模型优化器\n",
    "        self.optimizer_feat = torch.optim.Adam([self.feat_syn], lr=args.lr_feat)\n",
    "        self.optimizer_pge = torch.optim.Adam(self.pge.parameters(), lr=args.lr_adj)\n",
    "        \n",
    "        # 打印synthetic graph的节点数和特征维度\n",
    "        print('adj_syn:', (n,n), 'feat_syn:', self.feat_syn.shape)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.feat_syn.data.copy_(torch.randn(self.feat_syn.size()))\n",
    "\n",
    "    def loadCSV(self,path):\n",
    "        import csv\n",
    "        from collections import Counter\n",
    "        dictLabels = {}\n",
    "\n",
    "        # dictLabels（标签到子图的映射）、\n",
    "\n",
    "        with open(path, 'r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter=',')\n",
    "            next(csvreader, None)  # skip (filename, label)\n",
    "            for i, row in enumerate(csvreader):\n",
    "                filename = row[1]\n",
    "                g_idx = int(filename.split('_')[0])\n",
    "                label = row[2]\n",
    "                # append filename to current label\n",
    "\n",
    "                if label in dictLabels.keys():\n",
    "                    dictLabels[label].append(filename)\n",
    "                else:\n",
    "                    dictLabels[label] = [filename]\n",
    "\n",
    "        return dictLabels\n",
    "\n",
    "    def generate_labels_from_csv(self,file_path,ratio):\n",
    "        import csv\n",
    "        from collections import Counter\n",
    "        with open(file_path, 'r') as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "            data = list(reader)\n",
    "\n",
    "        counter = Counter()  # 使用 Counter 对标签进行计数\n",
    "        num_class_dict = {}  # 存储每个类别需要生成的节点数\n",
    "        n = len(data)  # 数据总数\n",
    "\n",
    "        # 统计每个类别的出现次数\n",
    "        for row in data:\n",
    "            label = int(row['label'])\n",
    "            counter[label] += 1\n",
    "\n",
    "        # 根据标签出现次数进行排序\n",
    "        sorted_counter = sorted(counter.items(), key=lambda x: x[1])\n",
    "        sum_ = 0\n",
    "        labels_syn = []  # 合成标签列表\n",
    "        syn_class_indices = {}  # 记录每个类别的索引范围\n",
    "    #     ratio = 0.08\n",
    "        # 计算每个类别需要生成的节点数，并生成合成标签\n",
    "        for ix, (c, num) in enumerate(sorted_counter):\n",
    "\n",
    "            if ix == len(sorted_counter) - 1:\n",
    "                num_class_dict[c] = int(n * ratio) - sum_  \n",
    "                # 如果是最后一个类别，那么直接计算需要生成的节点数，并将这个类别的合成标签添加到 labels_syn 列表中\n",
    "                syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "                labels_syn += [c] * num_class_dict[c]\n",
    "\n",
    "            else:\n",
    "                num_class_dict[c] = max(int(num * ratio), 1)  # 其他类别的节点数\n",
    "                # 如果不是最后一个类别，那么先计算需要生成的节点数，然后将这个类别的合成标签添加到 labels_syn 列表中。\n",
    "                sum_ += num_class_dict[c]\n",
    "                syn_class_indices[c] = [len(labels_syn), len(labels_syn) + num_class_dict[c]]\n",
    "                labels_syn += [c] * num_class_dict[c]\n",
    "\n",
    "        return n * ratio,labels_syn\n",
    "\n",
    "    def get_sub_adj_feat(self,features, labels_syn, label_dict):\n",
    "        idx_selected = []\n",
    "\n",
    "        from collections import Counter\n",
    "        counter = Counter(labels_syn.cpu().numpy())\n",
    "        selected_features = []\n",
    "        for c in counter.keys():\n",
    "            class_label = str(c)\n",
    "            if class_label in label_dict:\n",
    "                node_indices = [label_dict[class_label].index(node) for node in label_dict[class_label]]\n",
    "                num_features = counter[c]\n",
    "                selected_indices = np.random.choice(node_indices, size=num_features, replace=False)\n",
    "\n",
    "                idx_selected.extend(selected_indices)\n",
    "\n",
    "                selected_features.extend(features[selected_indices])\n",
    "\n",
    "        selected_features = np.array(selected_features)\n",
    "\n",
    "        return selected_features\n",
    "    \n",
    "    def generate_syn_feat_db(self,dictLabels,feat):\n",
    "\n",
    "#         dictLabels = loadCSV(path)\n",
    "        self.feat_syn = torch.from_numpy(self.get_sub_adj_feat(feat, self.labels_syn, dictLabels)).to(self.device)\n",
    "    # # 重置synthetic graph的特征\n",
    "    # reset_parameters()\n",
    "        adj_syn_orj = self.pge(self.feat_syn)\n",
    "        adj_syn_norm = utils.normalize_adj_tensor(adj_syn_orj, sparse=False)\n",
    "\n",
    "        adj_syn = adj_syn_norm\n",
    "        adj_syn[adj_syn < 0.5] = 0\n",
    "        adj_syn[adj_syn >= 0.5] = 1\n",
    "\n",
    "        adj_syn_norm_sp = sp.csr_matrix(adj_syn.cpu().detach().numpy())\n",
    "        g = dgl.from_scipy(adj_syn_norm_sp) # 生成的合成图太过密集，导致二跳子图约等于全图\n",
    "        # 将labels_syn转换为dictLabels形式的字典\n",
    "        dict_labels = {f\"0_{i}\": self.labels_syn[i].item() for i in range(len(self.labels_syn))}\n",
    "        db_syn = Subgraphs_syn('syn', dict_labels, n_way=self.args.n_way, k_shot=self.args.k_spt,k_query=self.args.k_qry, batchsz=100, args = self.args, adjs = g, h = self.args.h, labels = self.labels_syn)\n",
    "       \n",
    "\n",
    "        db = DataLoader(db_syn, self.args.task_num, shuffle=True, num_workers=self.args.num_workers, pin_memory=True, collate_fn = collate)\n",
    "        \n",
    "        return db,adj_syn_orj\n",
    "    \n",
    "    def train(self, verbose=True,feat = None,dgl_graph = None):\n",
    "        # 获取参数和数据\n",
    "        args = self.args\n",
    "        # data = self.data\n",
    "        device = self.device\n",
    "        db_train = self.db_train\n",
    "        db_val = self.db_val\n",
    "        db_test = self.db_test\n",
    "        max_acc = 0\n",
    "        \n",
    "        for out_loop in range(5):\n",
    "            self.maml = Meta(args, self.config,device).to(device)\n",
    "            model_parameters = list(self.maml.parameters())\n",
    "            # 重置synthetic graph的特征\n",
    "            self.reset_parameters()\n",
    "\n",
    "            for epoch in range(self. args.epoch):\n",
    "                db = DataLoader(db_train, args.task_num, shuffle=True, num_workers=args.num_workers, pin_memory=True, collate_fn = collate)\n",
    "                db_syn,adj_syn  = self.generate_syn_feat_db(self.dictLabels,feat[0])\n",
    "                db_iter = iter(db)  # Create an iterator for the DataLoader object\n",
    "                db_syn_iter = iter(db_syn) \n",
    "                iters = min(len(db),len(db_syn))\n",
    "                for step in range(iters):\n",
    "                    # Extract one batch of data from db\n",
    "                    x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry = next(db_iter)\n",
    "\n",
    "                    nodes_len = 0              \n",
    "                    nodes_len += sum([sum([len(j) for j in i]) for i in n_spt])\n",
    "                    losses_opt,loss_q = self.maml.grad_opt(x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry, feat)\n",
    "\n",
    "                    x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry = next(db_syn_iter)\n",
    "                    nodes_len = 0             \n",
    "                    nodes_len += sum([sum([len(j) for j in i]) for i in n_spt])\n",
    "                    losses_opt_syn,loss_q_syn = self.maml.grad_opt_syn(x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry, feat)\n",
    "\n",
    "                    print(\"gather gradients of parameters done\")\n",
    "                    # 计算梯度匹配损失\n",
    "                    len_opt = len(losses_opt)\n",
    "                    len_syn = len(losses_opt_syn)\n",
    "                    len_gw = min(len_syn,len_opt )\n",
    "                    # # 对于更长的一个，从后面开始截取\n",
    "                    # if len_opt > len_syn:\n",
    "                    #     losses_opt = losses_opt[len_opt - len_gw:]\n",
    "                        \n",
    "                    # elif len_syn > len_opt:\n",
    "                    #     losses_opt_syn = losses_opt_syn[len_syn - len_gw:]\n",
    "                        \n",
    "                    # print('len_opt:',len_opt)\n",
    "                    # print('len_syn:',len_syn)\n",
    "                    \n",
    "                    for i in range(len_gw):\n",
    "                        loss = torch.tensor(0.0,requires_grad=True).to(self.device)\n",
    "                       \n",
    "                        gw_real = torch.autograd.grad(losses_opt[i], model_parameters,retain_graph=True)\n",
    "                        \n",
    "                        gw_syn  = torch.autograd.grad(losses_opt_syn[i], model_parameters, create_graph=True,retain_graph=True)\n",
    "\n",
    "                        if i == 0:\n",
    "                            print(gw_syn)\n",
    "                            old_syn = gw_syn\n",
    "                            old_real = gw_real\n",
    "                        else:\n",
    "                            print(\"zy521\",all(torch.allclose(old, new) for old, new in zip(gw_real, old_real)))# 假设有两个梯度列表 gw_real1 和 gw_real2\n",
    "\n",
    "                            print(\"ztl521\",all(torch.allclose(old, new) for old, new in zip(gw_syn, old_syn)))\n",
    "                            \n",
    "                        gw_real = list((_.detach().clone() for _ in gw_real))\n",
    "                        gw_syn  = list((_.detach().clone() for _ in gw_syn))\n",
    "\n",
    "                        loss += match_loss(gw_syn, gw_real, args, device=self.device)\n",
    "                            \n",
    "                        # 计算正则化损失\n",
    "                        # TODO: regularize\n",
    "                        if args.alpha > 0:\n",
    "                            loss_reg = args.alpha * regularization(adj_syn, utils.tensor2onehot(self.labels_syn))\n",
    "                        # else:\n",
    "                        else:\n",
    "                            loss_reg = torch.tensor(0)\n",
    "\n",
    "                        loss = loss + loss_reg\n",
    "\n",
    "                        # 更新合成图\n",
    "                        self.optimizer_feat.zero_grad()\n",
    "                        # 每次计算梯度时，梯度都会被累加到梯度缓存中。\n",
    "                        # 因此，在每次更新模型参数之前需要将梯度缓存清零，以避免梯度累加的影响。\n",
    "                        self.optimizer_pge.zero_grad()\n",
    "                        loss.backward() # 计算损失函数对于模型参数的梯度\n",
    "\n",
    "                        # 根据 it 的值选择更新 self.optimizer_pge 或 self.optimizer_feat\n",
    "                        if i % 2 == 0:\n",
    "                            self.optimizer_pge.step() # 使用优化算法来更新模型参数\n",
    "                        else:\n",
    "                            self.optimizer_feat.step()\n",
    "\n",
    "                        if i % 5 ==0: # 打印梯度匹配损失\n",
    "                            print('Gradient matching loss:', loss.item())\n",
    "\n",
    "                    loss = torch.tensor(0.0,requires_grad=True).to(self.device)\n",
    "                    gw_real = torch.autograd.grad(loss_q, model_parameters)\n",
    "                    gw_real = list((_.detach().clone() for _ in gw_real))\n",
    "                    gw_syn  = torch.autograd.grad(loss_q_syn, model_parameters,create_graph=True)\n",
    "                    gw_syn  = list((_.detach().clone() for _ in gw_syn))\n",
    "                    loss += match_loss(gw_syn, gw_real, args, device=self.device)\n",
    "\n",
    "                    # 计算正则化损失\n",
    "                    # TODO: regularize\n",
    "                    if args.alpha > 0:\n",
    "                        loss_reg = args.alpha * regularization(adj_syn, utils.tensor2onehot(self.labels_syn))\n",
    "                    # else:\n",
    "                    else:\n",
    "                        loss_reg = torch.tensor(0)\n",
    "\n",
    "                    loss = loss + loss_reg\n",
    "\n",
    "                    # 更新合成图\n",
    "                    self.optimizer_feat.zero_grad()\n",
    "                    self.optimizer_pge.zero_grad()\n",
    "                    loss.backward() # 计算损失函数对于模型参数的梯度\n",
    "\n",
    "                    self.optimizer_pge.step() # 使用优化算法来更新模型参数\n",
    "                    self.optimizer_feat.step()\n",
    "                    print('Gradient matching loss:', loss.item())\n",
    "                \n",
    "                    # 进行内循环，更新 GNN 模型的参数\n",
    "                    self.maml.meta_optim.zero_grad()\n",
    "                    loss_q_syn.backward()\n",
    "                    # # 注册回调函数保存梯度\n",
    "                    # self.net.parameters().register_hook(save_gradients)\n",
    "                    self.maml.meta_optim.step() # update gnn param\n",
    "\n",
    "                if epoch % 2 == 0:\n",
    "                    # validation per epoch\n",
    "                    db_v = DataLoader(db_val, 1, shuffle=True, num_workers=args.num_workers, pin_memory=True, collate_fn = collate)\n",
    "                    accs_all_test = []\n",
    "\n",
    "                    for x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry in db_v:\n",
    "\n",
    "                        accs = self.maml.finetunning(x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry, feat)\n",
    "                        accs_all_test.append(accs)\n",
    "\n",
    "                    accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "                    print('Epoch:', epoch + 1, ' Val acc:', str(accs[-1])[:5])\n",
    "                # if accs[-1] > max_acc:\n",
    "                #     max_acc = accs[-1]\n",
    "                #     model_max = copy.deepcopy(self.maml)\n",
    "    \n",
    "        db_t = DataLoader(db_test, 1, shuffle=True, num_workers=args.num_workers, pin_memory=True, collate_fn = collate)\n",
    "        accs_all_test = []\n",
    "\n",
    "        for x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry in db_t:\n",
    "            accs = self.maml.finetunning(x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry, feat)\n",
    "            accs_all_test.append(accs)\n",
    "\n",
    "        accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "        print('Test acc:', str(accs[1])[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj_syn: (251, 251) feat_syn: torch.Size([251, 128])\n"
     ]
    }
   ],
   "source": [
    "Meta_learner = Meta_learner(db_train, db_val, db_test, args, device,feat_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enable =True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather gradients of parameters done\n",
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1',\n",
      "       grad_fn=<AddBackward0>), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1',\n",
      "       grad_fn=<AddBackward0>), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), tensor([0., 0., 0.], device='cuda:1', grad_fn=<AddBackward0>))\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "gather gradients of parameters done\n",
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1',\n",
      "       grad_fn=<AddBackward0>), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1',\n",
      "       grad_fn=<AddBackward0>), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), tensor([0., 0., 0.], device='cuda:1', grad_fn=<AddBackward0>))\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "gather gradients of parameters done\n",
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1',\n",
      "       grad_fn=<AddBackward0>), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1',\n",
      "       grad_fn=<AddBackward0>), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), tensor([0., 0., 0.], device='cuda:1', grad_fn=<AddBackward0>))\n",
      "Gradient matching loss: 768.0\n",
      "zy521 True\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n",
      "gather gradients of parameters done\n",
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1',\n",
      "       grad_fn=<AddBackward0>), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1',\n",
      "       grad_fn=<AddBackward0>), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), tensor([0., 0., 0.], device='cuda:1', grad_fn=<AddBackward0>))\n",
      "Gradient matching loss: 768.0\n",
      "zy521 True\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "zy521 False\n",
      "ztl521 True\n",
      "Gradient matching loss: 768.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Meta_learner\u001b[39m.\u001b[39;49mtrain(verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,feat \u001b[39m=\u001b[39;49m feat,dgl_graph \u001b[39m=\u001b[39;49m dgl_graph)\n",
      "Cell \u001b[0;32mIn[5], line 309\u001b[0m, in \u001b[0;36mMeta_learner.train\u001b[0;34m(self, verbose, feat, dgl_graph)\u001b[0m\n\u001b[1;32m    305\u001b[0m accs_all_test \u001b[39m=\u001b[39m []\n\u001b[1;32m    307\u001b[0m \u001b[39mfor\u001b[39;00m x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry \u001b[39min\u001b[39;00m db_v:\n\u001b[0;32m--> 309\u001b[0m     accs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmaml\u001b[39m.\u001b[39;49mfinetunning(x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry, feat)\n\u001b[1;32m    310\u001b[0m     accs_all_test\u001b[39m.\u001b[39mappend(accs)\n\u001b[1;32m    312\u001b[0m accs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(accs_all_test)\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat16)\n",
      "File \u001b[0;32m~/ztl_project/Graph_DD/META-DD/G-Meta/meta.py:484\u001b[0m, in \u001b[0;36mMeta.finetunning\u001b[0;34m(self, x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry, feat)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinetunning\u001b[39m(\u001b[39mself\u001b[39m, x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry,feat):\n\u001b[1;32m    483\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mG-Meta\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 484\u001b[0m         accs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfinetunning_ProtoMAML(x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry, feat)\n\u001b[1;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m accs\n",
      "File \u001b[0;32m~/ztl_project/Graph_DD/META-DD/G-Meta/meta.py:461\u001b[0m, in \u001b[0;36mMeta.finetunning_ProtoMAML\u001b[0;34m(self, x_spt, y_spt, x_qry, y_qry, c_spt, c_qry, n_spt, n_qry, g_spt, g_qry, feat)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_step_test):\n\u001b[1;32m    459\u001b[0m     \u001b[39m# 1. run the i-th task and compute loss for k=1~K-1\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     logits, _ \u001b[39m=\u001b[39m net(x_spt\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), c_spt\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), feat_spt, fast_weights)\n\u001b[0;32m--> 461\u001b[0m     loss, _, prototypes \u001b[39m=\u001b[39m proto_loss_spt(logits, y_spt, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk_spt)\n\u001b[1;32m    462\u001b[0m     \u001b[39m# 2. compute grad on theta_pi\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     grad \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(loss, fast_weights, retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/ztl_project/Graph_DD/META-DD/G-Meta/meta.py:54\u001b[0m, in \u001b[0;36mproto_loss_spt\u001b[0;34m(logits, y_t, n_support)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39m# 获取每个类别的查询集索引，并计算查询样本与原型之间的欧几里得距离\u001b[39;00m\n\u001b[1;32m     53\u001b[0m query_idxs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m c: target_cpu\u001b[39m.\u001b[39meq(c)\u001b[39m.\u001b[39mnonzero()[:n_support], classes)))\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m query_samples \u001b[39m=\u001b[39m input_cpu[query_idxs]   \n\u001b[1;32m     55\u001b[0m dists \u001b[39m=\u001b[39m euclidean_dist(query_samples, prototypes)\n\u001b[1;32m     57\u001b[0m \u001b[39m# 通过log_softmax函数计算损失值和准确率，并返回损失值、准确率和原型 \u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Meta_learner.train(verbose=True,feat = feat,dgl_graph = dgl_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
